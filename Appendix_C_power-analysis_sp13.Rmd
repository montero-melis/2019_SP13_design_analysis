---
title: 'Appendix C: Power analysis for SP13 replication'
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: "`r as.character(Sys.Date())`"
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

This knitr script executes and reports a power analysis for our replication of
Shebani and Pulvermüller (2013, *Cortex*), henceforth SP13.

This document complements our manuscript submitted as a Registered Report to
*Cortex*. See end of document for session info, including package versions, etc.
and the manuscript for further context.


General approach
---------------

We adopt the following approach for our Bayes factor design analysis (see
Schönbrodt and Wagenmakers, 2017, *Psychon Bull Rev*):

1. Simulate random data from the statistical model fitted to the original data
and other parameters estimated from pilot data.
2. Fit two logisic mixed models to each data set using `lme4`, one that includes
the interaction of interest and another that does not.
3. Compute a Bayes factor for the two models based on the Bayesian information
   criterion (BIC) (see [this link](https://rpubs.com/lindeloev/bayes_factors) 
   for the exact approach).
4. Repeat steps 1-3 a large number of times for different sample sizes and for
a scenario in which the critical effect is zero (Type 1 simulations) and a 
scenario where the critical effect exists (Type 2 simulations).


About the parameters of our statistical model to generate the data
----------------------------------------

Data simulations are based on our re-analysis of the original data as shared by
SP13. Since that data set did not consist of trial-level observations, we do not
have a way to estimate item variability from it.
We therefore use our own pilot data to estimate item-level variability (details
below).
All in all, we assume a generative model for the simulations that mimicks
our own experimental setup. Each participant generates data for the four
experimental cells defined by our design:

- Word type: arm words vs leg words
- Inferference type: arm vs foot paradiddle

There are 104 items (52 arm and 52 leg words) in our study, so we simulate
208 observations per participant.
Each data set is randomly generated under a probabilistic binomial (Bernoulli)
hierarchical model in which the log-odds of producing an error are a function
of the population-level (fixed) effects predictors Interference Movement
(arm movements vs. leg movements), Word Type (arm-related vs leg-related words),
and their interaction. In addition, random effects variance was added by
participants (for intercepts and all the fixed effects and interaction slopes)
and items (for intercepts and slopes for Interference Movement). The simulations
cross the following factors:

1) Participant sample size: N=15, 60, 108; that is, the original sample size, 
Nmin, and Nmax, respectively.
2) Simulation type: Type 1 (critical population-level effect set to zero), type 2
(critical population-level effect sampled from the model of the original data).



Justification of BIC approach
-----------------------------

There is a practical reason we do not run simulations using the fully Bayesian
approach we will use for our main analysis:
It takes roughly 2 hours to run each analysis of a simulated data set using
`brms` and bridge sampling (as implemented in our analysis pipeline, see
[Appendix_E_analysis_pipeline](https://github.com/montero-melis/2019_SP13-replication_reg-report/blob/master/Appendix_E_analysis_pipeline.html)).
If we used this approach to simulate 10000 data sets for three different Ns, 
and the two simulation types, it would take approximately 
$10000 \times 3 \times 2 = 60000$ hours,
or almost *seven years* to run the simulations.
Our approach is a compromise that allows for a good enough estimation of Bayes
factors taking into account the specifics of our design as opposed to 
off-the-shelve R packages like `BFDA` (Schönbrodt & Stefan, 2018).


Setup workspace
===============

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library("knitr")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("broom")
library("boot")       # for inv.logit()
library("lme4")
library("brms")
library("tictoc")
library("ggpubr")
library("foreach")     # to parallelize simulations
library("doParallel")  # to parallelize simulations
```

```{r}
# Global plotting parameters
myalpha <- 0.02
theme_set(theme_bw())
```


```{r}
# Load original data from SP13
d <- read_csv("data/SP13_orig-data_total-errors_long-format.csv") %>%
  # keep critical interference conditions only
  filter(movement %in% c("arm_paradi", "leg_paradi"))
# For each experimental cell, the maximum number of errors is 48 (see
# "Appendix_B_reanalysis_original.Rmd" for justification0)
d$n <- 48
```


Model of original data 
======================

The model takes some time to fit with `brms`, so retrieve it from disk:

```{r}
# Load from disk - list containing 1) full model, 2) null model, 3) BF
bfm_max <- read_rds("sims_etc/sp13_bfm_max.rds")
```

```{r}
bfm_full <- bfm_max[[1]]
summary(bfm_full)
```


Bayes Factor for H1 vs H0:

```{r}
bfm_max[[3]]
```




Power simulations
=================

Prepare data generating process
-------------------------------

### Load function

Load the **function** that generates binomial data for our design.

```{r}
source("generate_data_fnc.R")
```


### Set up simulation parameters

The output from the model fit to the original data serves as input to the function.
We need to put them into the right format:

```{r}
# The info we need is in the summary of fixed effects of the model:
fixef(bfm_full)

# Vector of coefficient means
fixef_means <- fixef(bfm_full)[, 1]
fixef_means

# Covariance matrix (Sigma); we need to square the SEs to convert them to Variances
fixef_sigma <- diag(fixef(bfm_full)[, 2] ^ 2)  # we assume uncorrelated diagonal matrix
fixef_sigma

# Random effects for the model:
VarCorr(bfm_full)$subject$sd
# Specifically we extract by-subject random SDs from this matrix (they are stored
# as a "stddev" attribute of the matrix) and square them to obtain variances:
ranef_sigma_subj <- diag(VarCorr(bfm_full)$subject$sd[, 1]) ^ 2
ranef_sigma_subj
```

An estimate for random effects by item we take from our pilot data (see
[here](https://github.com/montero-melis/2018_replication_sheb-pulv2013/blob/master/1806_pilot_analysis/item-variance_estimates.R)).


```{r}
# The random by-item intercept is SD=0.65. However, we don't have estimates for
# by-item random slope for movement because we only ran the control condition in
# the pilot. We will assume that random slope to be the same as the by-subject
# random variability for the critical interaction, which is likely to be an
# overestimate but then it will only make our power estimates more conservative:
ranef_sigma_item <- diag(c(0.65, 0.06)) ^ 2  # square to obtain variances
ranef_sigma_item
```


### Example simulation

We can now call the `simulate_binom` function to generate data:

```{r, message = FALSE}
sim_ex <- simulate_binom(
  Nsubj = 10,
  Nitem = 20,
  fixef_means = fixef_means,
  fixef_sigma = fixef_sigma,
  ranef_sigma_subj = ranef_sigma_subj,
  ranef_sigma_item = ranef_sigma_item
  )
sim_ex
```



Functions to analyze simulations
--------------------------------

### Basic function

```{r}
analyze_simulation <- function(df, print_model_summaries = FALSE) {

  # start counter
  tic()

  # Contrast coding (umeric coding allows for models without RE correlations)
  df$mv <- scale(as.numeric(df$movement), scale = FALSE) * -2
  df$wt <- scale(as.numeric(df$word_type), scale = FALSE) * -2
  df$mv_wt <- df$mv * df$wt

  # function to catch convergence issues
  converge <- function (fm) {
  # Paste in case there are several messages
  message <- paste(summary(fm)$optinfo$conv$lme4$messages, collapse = " / ")
  if( is.null(message) ) { "" } else { message }
  }
  
  # Function to fit a full and null model given model formula as string:
  fit_formula <- function (f_str, d = df) {
    formula <- as.formula(f_str)  # convert to formula
    full <- glmer(formula, data = d,
                  control = glmerControl(optCtrl=list(maxfun=1e6)),
                  family = "binomial"
                  )
    null <- update(full, formula = ~ . - mv_wt)

    if (print_model_summaries) {
      print(summary(full))
      print(summary(null))
    }
    list(full, null)
  }
  
  # Fit models without RE correlations using double-bar syntax
  # (see Reinhold Kliegl's post: https://rpubs.com/Reinhold/391071)
  myf <- "Error ~ mv + wt + mv_wt + (1 + mv + wt + mv_wt || subject) + (1 + mv || item)"
  models <- fit_formula(myf)

  # Put fixed effect model coefficients into one data frame
  model_info <- bind_rows(
    unnest(tidy(models[[1]])) %>%
      mutate(model = "full", convergence = converge(models[[1]])),
    unnest(tidy(models[[2]])%>% filter(group == "fixed" | grepl("mv_wt", term))) %>%
      mutate(model = "null", convergence = converge(models[[2]]))
  ) %>%
    # Now add more concise info (redundantly repeated across rows, can be filtered
    # out later):
    mutate(
      BIC_full = BIC(models[[1]]),
      BIC_null = BIC(models[[2]]),
      BF_BIC = exp( (BIC(models[[2]]) - BIC(models[[1]]) ) / 2),
      conv_full = converge(models[[1]]),
      conv_null = converge(models[[2]]),
      formula_full = myf
    ) %>%
    select(-group)
  # Add the time it took
  mytoc <- toc()
  model_info$t_ellapsed <- mytoc$toc - mytoc$tic
  model_info
}
```


Let's illustrate with an example.

```{r, message=FALSE, warning=FALSE}
set.seed(753)
sim_ex <-  simulate_binom(
  Nsubj = 20,
  Nitem = 10,
  fixef_means = fixef_means,
  fixef_sigma = fixef_sigma,
  ranef_sigma_subj = ranef_sigma_subj,
  ranef_sigma_item = ranef_sigma_item,
  )
tictoc::tic()
# print_model_summaries=TRUE would print the actual model summaries to screen
an_ex <- analyze_simulation(sim_ex, print_model_summaries = FALSE)
tictoc::toc()
```

The output is a data frame with information about the analysis of the simulated
data set. Note that the first columns are unique (they are about parameter
estimates in the model), but starting from the convergence column the info is
repeated across rows. We will mostly use results from the BIC of every model
but all the other output is good to keep to better understand what is going on
and what the actual parameter estimates are for the different models:

```{r}
kable(head(an_ex, 3))
```




### Function to analyze many data sets

This function does the basic work: It is designed to be applied with
`purrr::pmap` together with a data frame where each row passes the relevant
parameters that vary across simulations (Nsubj and sim_type) and some other
more detailed stuff (e.g., rseed, incl_col_names, ...).

Note that it is designed to directly write simulation results to a file on disk,
appending to whatever already is on the file. This is because the simulations
can take days or weeks and this makes it possible to break the process at some
point and resume later.


```{r}
sim_many <- function(rseed, sim_id, Nsubj, sim_type, incl_col_names, ...) {
  istype1 <- if (sim_type == "type2") {
    FALSE
    } else if (sim_type == "type1") {
    TRUE
    } else{
        stop("sim_type has to be either 'type1' or 'type2'")
    }
  set.seed(rseed)
  df <- simulate_binom(Nsubj, type1 = istype1, ...)
  result <- analyze_simulation(df)
  result$sim_id   <- sim_id
  result$rseed    <- rseed
  result$sim_type <- sim_type
  result$Nsubj    <- Nsubj
  write_csv(
    result,
    path = "power_simulation_results_append.csv",
    col_names = incl_col_names,  # only needed first time it's run
    append = TRUE)
  result
}
```


### Function to tally the simulations left to run

A function that computes how many simulations we still need to run, based on
the file on disk that stores simulation results. It outputs FALSE if it doesn't
find the file. This function is called from the `run_till_aim()` function (below)
which actually runs the simulations.

```{r}
sim_till_aim <- function(
  aim,
  filter_out_nsubj = 0,  # by default this will consider all sample sizes
  filter_out_type = ""   # by default it will consider type1 and type2
  ) {
  neg2zero <- function(x) ifelse(x < 0, 0, x)  # little trick needed below
  # retrieve simulations and check which ones converged (both null and full)
  if (file.exists("power_simulation_results_append.csv")) {
    sims <- read_csv("power_simulation_results_append.csv")
  } else {
    return (FALSE)
  }
  # If there are simulations, get the right info about convergence etc
  sims <- sims %>%
    select(rseed, Nsubj, sim_type, conv_full, conv_null) %>% unique() %>%
    mutate(
      converged = ( is.na(conv_full) & is.na(conv_null) ),
      Nsubj = as.numeric(Nsubj)
      ) %>%
    group_by(Nsubj, sim_type) %>%
    summarise(sims_run = n(), converged = sum(converged)) %>%
    mutate(
      aim = aim,
      needed_neg = aim - converged,
      needed = neg2zero(needed_neg)) %>%
    # Estimated convergence rate at different sample sizes
    left_join(tibble(
      Nsubj =     c(15,   60,   96,   102,  108),
      conv_rate = c(0.13, 0.40, 0.55, 0.56, 0.59))
      ) %>%
    mutate(nsims = round(needed / conv_rate)) %>%
    filter(
      (! Nsubj %in% filter_out_nsubj),
      (sim_type != filter_out_type)
      )
  sims
}
```



### Wrapper function

We need a wrapper function that generates data sets with different simulation
parameters (different sample sizes and either Type 1 or Type 2 simulations),
analyzes the data sets and stores the result. This function will run until
a pre-specified aim of *converged* models. (Note that models may not always 
converge, which is especially frequent when sample sizes are small).

Note that it doesn't generate output to the console. Instead it writes the 
results to file (because it calls `sim_many()` and that is what `sim_many()`
does).

```{r}
# Function checks how many converged simulations there are and runs new
# ones until aim is reached. Note the function is parallelized to run faster.
run_till_aim <- function(
  aim,  # the number of converged models we want for each sample size (type1/2)
  sample_sizes = c(15, 60, 108),
  nb_cores = parallel::detectCores(),  # Number of cores used in parallelization
  ...
  ) {
  tictoc::tic()  # start counter
  sims <- sim_till_aim(aim, ...)  # nb of simulations left

  # 1st case: no simulations have been run, in which case sims is FALSE
  if (is.logical(sims)) {
    # so then run we'll run one of each sample size to get started
    print("Create file and run one type1 and type2 sim for each sample size")
    params <- tibble(
      Nsubj = rep(sample_sizes, each = 2),
      sim_type = rep(c("type2", "type1"), length.out = 2 * length(sample_sizes))
      )
    # include col names 1st time
    params$incl_col_names <- c(TRUE, rep(FALSE, nrow(params) - 1))
    params$rseed <- sample.int(10 ^ 9, size = nrow(params))  # random seeds
    params$sim_id <- seq_len(nrow(params))
    print("We'll now run:")
    print(params)
    # We can't parellize it yet, bc we want column names to be in row 1:
    purrr::pmap(
      .l = params,
      .f = sim_many,
      # fixed simulation parameters (for clarity, comment out those passed in params)
      # Nsubj =,
      Nitem = 104,
      fixef_means = fixef_means,
      fixef_sigma = fixef_sigma,
      ranef_sigma_subj = ranef_sigma_subj,
      ranef_sigma_item = ranef_sigma_item
    )

    } else {  # So there is already a file...
      # Check if we have attained the aim
      if (sum(sims$nsims) == 0) {
        return("Done, my friend!!!")
      }
      # There are simulations but we haven't reached the aim.
      # Print how many simulations will be run:
      print("We'll now run:")
      print(sims)
      # set up parameters
      params <- tibble(
        rseed = sample.int(10 ^ 9, size = sum(sims$nsims)),
        Nsubj = rep(sims$Nsubj, sims$nsims),
        sim_type = rep(sims$sim_type, sims$nsims),
        incl_col_names = FALSE
      )
      params$sim_id <- seq_len(nrow(params))

      # Now we can parallelize!
      registerDoParallel(nb_cores)  # use multicore, by default uses all available
      foreach (
        i = 1:nrow(params),
        .packages = c("tidyverse", "broom", "lme4", "mvtnorm", "boot", "tictoc"),
        .export = c(
          "sim_many", "simulate_binom", "analyze_simulation", "fixef_means",
          "fixef_sigma", "ranef_sigma_subj", "ranef_sigma_item"
          )
        ) %dopar% {
          purrr::pmap(
            .l = params[i,],
            .f = sim_many,
            # fixed simulation parameters (for clarity, comment out those passed in params)
            # Nsubj =,
            Nitem = 104,
            fixef_means = fixef_means,
            fixef_sigma = fixef_sigma,
            ranef_sigma_subj = ranef_sigma_subj,
            ranef_sigma_item = ranef_sigma_item
          )
        }
      }
  tictoc::toc()  # stop counter
  # recursively runs itself until aim is reached
  run_till_aim(aim, ...)
}
```


Run the simulations (generate and analyze data sets)
-----------------------------------------------

The following code will run the simulations until there are as many converged
simulations of each type as specified in the `aim` argument.
Note that running 1000 simulations could take several days on a normal computer.
Running 10 000 as we did is only recommended on a computer cluster.

```{r, message=FALSE}
sim_till_aim(1)  # Outputs FALSE if there isn't any file on disk yet
# run_till_aim(aim = 1)
```




Load simulations from disk
--------------------------

Once the simulations have run, we load results from disk:

```{r, message = FALSE}
# This version of the file keeps stacking the result of simulations:
# mysims <- read_csv("sims_etc/power_simulation_results_append.csv")
mysims <- read_csv("sims_etc/power_simulation_results_append_lite.csv")
# Rearrange columns for convenience:
mysims <- mysims %>%
  select(sim_id : Nsubj, t_ellapsed, term : conv_null)
head(mysims, 3) %>% kable(digits = 2)
```





Analyze simulation results
===============


Divide simulation results into two data frames
------------------------------

The data frame loaded above contains a lot of information about the model
fitting (it consists of `r nrow(mysims)` rows).
Mainly it consists of two sets of output:

1) The BIC of the models and the corresponding Bayes factor (BF_BIC):

```{r}
sims_BIC <- mysims %>%
  select(sim_id : t_ellapsed, BIC_full : conv_null) %>%
  unique() %>%
  mutate(
    converged = is.na(conv_full) & is.na(conv_null)
    )
sims_BIC %>% head(3) %>% kable(digits = 2)
```

2) The fixed effect coefficient estimates of each model (in long format):

```{r}
sims_coef <- mysims %>%
  select(sim_id : convergence) %>%
  unique() %>%
  mutate(converged = is.na(convergence) | convergence == "")
sims_coef %>% head(3) %>% kable(digits = 2)
```


Number of simulations and convergence failures
---------------------

Many models failed to converge, especially those with sample sizes of 15.
We are only interested in simulations in which both full and null model converged.
What's the percentage of convergence failures?

```{r}
sims_BIC %>%
  group_by(Nsubj, sim_type) %>%
  summarise(run = n(), converged = sum(converged)) %>%
  mutate(prop_converged = converged / run) %>%
  kable(digits = 2)
```

Keep converged models only
--------------------------

We keep only the models that converged. In an appendix below we show that this
choice makes our estimates more *conservative*.

```{r}
sims_BIC_all <- sims_BIC
sims_BIC     <- sims_BIC %>% filter(converged == TRUE)
sims_coef_all <- sims_coef
sims_coef     <- sims_coef %>% filter(converged == TRUE)
```


Bayes factors
-------------

### Actual BFs

Visual summary of results (BFs on log10 scale):

```{r}
ggplot(sims_BIC, aes(x = factor(Nsubj), y = log10(BF_BIC))) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(. ~ sim_type) +
  geom_hline(yintercept = log10(6), linetype = "dashed", colour = "red") +
  geom_hline(yintercept = log10(1/6), linetype = "dashed", colour = "blue") +
  xlab("Sample size (number of participants)")
```

Numerical summary

```{r}
by(sims_BIC$BF_BIC, with(sims_BIC, list(Nsubj, sim_type)), summary)
```


### Design analysis: How often will we reach the right decision?

```{r}
plot_sims <- function(type, df = sims_BIC) {
  d <- sims_BIC %>%
    filter(sim_type == type, Nsubj %in% c(15, 60, 108), converged == TRUE) %>%
    select(rseed : Nsubj, BF_BIC) %>%
    unique() %>%
    mutate(BF_decision = ifelse(
      BF_BIC >= 6, "Accept H1", ifelse(
        BF_BIC <= 1/6, "Accept H0", "Inconclusive"))
      ) %>%
    group_by(Nsubj, BF_decision) %>%
    summarise(count = n()) %>%
    mutate(perc = count / sum(count))

  # Assign a bunch of values depending on whether it's type 1 or 2 simulations
  mylevels <- c("Accept H0", "Inconclusive", "Accept H1")
  # mycolours <- c("#E69F00", "#999999", "#56B4E9")
  mycolours <- c("#00AFBB", "#E7B800", "#FC4E07")

  if (type == "type2") {
    mytitle <- "Type 2 simulations: H1 is true\n(effect is present)"
    yinterc <- 0.9
    } else if (type == "type1") {
      mytitle <- "Type 1 simulations: H0 is true\n(effect is absent)"
      yinterc <- 0.05
      } else {
        stop("Invalid argument for type!")
      }
  d$BF_decision <- factor(d$BF_decision, levels = mylevels)
  # Plot it
  p <- ggplot(d, (aes(x = factor(Nsubj), y = perc, fill = BF_decision))) +
    geom_bar(stat = "identity") +
    # Don't drop unused levels
    scale_fill_manual(values = mycolours, drop = FALSE) +
    geom_hline(yintercept = yinterc, linetype = "dashed") +
    xlab("Number of participants") +
    ylab("Proportion") +
    labs(fill = "Decision") +
    ggtitle(mytitle) +
    theme(legend.position = "bottom")
  p
}
```

```{r, fig.width=8}
p1 <- plot_sims("type1")
p2 <- plot_sims("type2")
ggarrange(p1, p2, ncol=2, nrow=1, common.legend = TRUE, legend="right")
```

In table format:

```{r}
sims_BIC %>%
  filter(Nsubj %in% c(15, 60, 108), converged == TRUE) %>%
  select(rseed : Nsubj, BF_BIC) %>%
  unique() %>%
  mutate(BF_decision = ifelse(
    BF_BIC >= 6, "Accept H1", ifelse(
      BF_BIC <= 1/6, "Accept H0", "Inconclusive"))
    ) %>%
  group_by(sim_type, Nsubj, BF_decision) %>%
  summarise(count = n()) %>%
  mutate(percentage = 100 * count / sum(count)) %>%
  kable(digits = 1)
```



Significance of critical coefficients
-------------------------------------

Summary of z-values for critical interaction (in full models).
The dashed line marks the conventional threshold for significance in frequentist
analysis (alpha = .05).

```{r}
sims_coef %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = factor(Nsubj), y = statistic)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(. ~ sim_type) +
  geom_hline(yintercept = 2, linetype = "dashed") +
  xlab("Sample size (number of participants)")
```


Sanity check: Are we retrieving the correct estimates?
-----------------------------------------------------

Since we are simulating the data sets, we *know* what the population-level
effects are, both for fixed and random effects. Are our models correctly
retrieving them?

First, estimates of the population-level effect for the critical interaction:

```{r}
sims_coef %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = factor(Nsubj), y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(. ~ sim_type) +
  geom_hline(yintercept = fixef_means[4], linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "blue") +
  ylab("Estimate")
```


Estimates of the random by-subject variation for critical effect:

```{r}
sims_coef %>%
  filter(grepl("sd_mv_wt\\.subject", term)) %>%
  ggplot(aes(x = factor(Nsubj), y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(. ~ sim_type) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[4]), linetype = "dashed")
```

Estimates of the random by-subject variation for the intercept:

```{r}
sims_coef %>%
  filter(grepl("sd_\\(Intercept\\).subject", term)) %>%
  ggplot(aes(x = factor(Nsubj), y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(. ~ sim_type) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[1]), linetype = "dashed")
```




Comparison of methods -- why are BF_BICs so high?
================================================

BFs based on BICs are humongous. Is it a consequence of the specific method
to compute these BFs?


Approach
--------

Let's take one generated data set and analyze it using three methods:

1. Intercept only `lme4` model
2. Maximal REs `lme4` model
3. Maximal REs `brms` model and bridge sampling to obtain BFs (as we will do in
our actual data set)

The data set
-----------

Take a data set that doesn't fail to converge for N=100 and is around the median:

```{r}
# sims_BIC %>%
#   filter(converged == TRUE & Nsubj == 96) %>%
#   arrange(BF_BIC) %>%
#   kable
```

We take simulation 29 with a BF of around a billion.

```{r}
# set.seed(158912513)
# df_sim <- simulate_binom(
#   Nsubj = 100,
#   Nitem = 104,
#   fixef_means = fixef_means,
#   fixef_sigma = fixef_sigma,
#   ranef_sigma_subj = ranef_sigma_subj,
#   ranef_sigma_item = ranef_sigma_item
#   )
# df_sim
```


Adjust coding scheme:

```{r}
# # Get the coding scheme right:
# contrasts(df_sim$movement)  <- contr.sum(2)
# contrasts(df_sim$movement)
# contrasts(df_sim$word_type) <- contr.sum(2)
# contrasts(df_sim$word_type)
```




Analyze with 3 methods
----------------------

### Intercepts only lme4

```{r}
# fm_interc_full <-  glmer(
#   Error ~ movement * word_type +
#     # (1 + movement * word_type | subject) +
#     # (1 + movement | item),
#     (1 | subject) +
#     (1 | item),
#   data = df_sim, family = "binomial"
# )
# # Without fixef interaction term:
# fm_interc_null <- update(fm_interc_full, formula = ~ . - movement : word_type)
```

BICs and BIC-based BF:

```{r}
# BIC(fm_interc_full)
# BIC(fm_interc_null)
# # BF
# exp( (BIC(fm_interc_null) - BIC(fm_interc_full) ) / 2)
```

Model summaries

```{r}
# summary(fm_interc_null)
# summary(fm_interc_full)
```


### Maximal lme4

```{r}
# tic()
# fm_max_full <-  glmer(
#   Error ~ movement * word_type +
#     (1 + movement * word_type | subject) +
#     (1 + movement | item),
#     # (1 | subject) +
#     # (1 | item),
#   data = df_sim, family = "binomial"
# )
# toc()
# tic()
# # Without fixef interaction term:
# fm_max_null <- update(fm_max_full, formula = ~ . - movement : word_type)
# toc()
```

BICs and BIC-based BF:

```{r}
# BIC(fm_max_full)
# BIC(fm_max_null)
# # BF
# exp( (BIC(fm_max_null) - BIC(fm_max_full) ) / 2)
```

Model summaries

```{r}
# summary(fm_max_null)
# summary(fm_max_full)
```


### Maximal with brms

```{r}
# myprior <- set_prior("normal(0, 2)", class = "b")
# print(myprior)
# # Model *without* interaction [took 3.3h]
# tic()
# bfm_null <- brm(
#   Error ~
#     1 + movement + word_type +  # critical manipulations but no interaction
#     (1 + movement * word_type | subject) + (1 + movement | item),  # maximal random structure
#   data = df_sim,
#   prior = myprior,
#   family = "bernoulli",
#   iter = 15000, warmup = 2000, chains = 4,  # https://discourse.mc-stan.org/t/bayes-factor-using-brms/4469/3
#   save_all_pars = TRUE  # necessary for brms::bayes_factor() later
# )
# toc()
# # fit full model (with interaction):
# tic()
# bfm_full <- brm(
#   Error ~
#     1 + movement * word_type +  # critical manipulations and interaction
#     (1 + movement * word_type | subject) + (1 + movement | item),  # maximal random structure
#   data = df_sim,
#   prior = myprior ,
#   family = "bernoulli",
#   iter = 15000, warmup = 2000, chains = 4,  # https://discourse.mc-stan.org/t/bayes-factor-using-brms/4469/3
#   save_all_pars = TRUE  # necessary for brms::bayes_factor() later
# )
# toc()
```

```{r}
# summary(bfm_null)
# summary(bfm_full)
```


BF using bridge sampling:

```{r}
# BF_bfm <- brms::bayes_factor(bfm_full, bfm_null)
# BF_bfm
```


### Save it all to disk

```{r}
# model_list <- list(
#   fm_interc_null, fm_interc_full, fm_max_null, fm_max_full, 
#   bfm_null, bfm_null, BF_bfm
#   )
# # write_rds(model_list, path = "sims_etc/model_compare_158912513.Rda")
```


Compare the three methods
------------------------

```{r}
# model_list1 <- read_rds("sims_etc/model_compare_158912513.Rda")
# model_list2 <- read_rds("sims_etc/model_compare_189941429.Rda")
```


Function

```{r}
# compare_models_BIC <- function (model_list) {
# 
#   p <- function(x) print(x)
# 
#   p("fm_interc_null")
#   p(BIC(model_list[[1]]))
#   p("fm_interc_full")
#   p(BIC(model_list[[2]]))
#   p("BIC_BF")
#   p(exp( (BIC(model_list[[1]]) - BIC(model_list[[2]]) ) / 2))
#   cat("\n")
# 
#   p("fm_max_null")
#   p(BIC(model_list[[3]]))
#   p("fm_max_full")
#   p(BIC(model_list[[4]]))
#   p("BIC_BF")
#   p(exp( (BIC(model_list[[3]]) - BIC(model_list[[4]]) ) / 2))
#   cat("\n")
# 
#   p("BF bridge sampling with brms model")
#   p(model_list[[7]])
# }
# 
# model_summaries <- function(model_list) {
#   p <- function(x) print(x)
#   p(summary(model_list[[2]]))
#   p(summary(model_list[[3]]))
#   p(summary(model_list[[4]]))
#   p(summary(model_list[[6]]))
# }
```




```{r}
# compare_models_BIC(model_list1)
# compare_models_BIC(model_list2)
# 
# model_summaries(model_list1)
# model_summaries(model_list2)
```


Conclusion (Revise this!!!)
==========

Our proposed design with $N_{min}=60$ and $N_{max}=96$ (step size = 12) and the
evidence threshold set at $BF_{10} > 6$ or $BF_{01} > 6$ will largely suffice to
guarantee high power (>90%) if the effect was as strong as reported in the
original study. 
Taking into account, however, that a) our re-analysis of the original data suggests
that the effect size might have been severely over-estimated, and b) the
original ANOVA-based analysis seem inappropriate for the data at hand, we have
chosen a design that will also guarantee  a high probability of compelling
evidence using an improved analysis method and with a small-to-medium effect size.



Appendix: Converged vs non-converged models
==========================================

We discarded all models that failed to converge or resulted in other model
fitting problems. Here we show that these models were *more liberal* in accepting
the H1. In other words, our choice of excluding them makes our results more
conservative:



```{r}
ggplot(sims_BIC_all, aes(x = converged, y = log10(BF_BIC))) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = log10(6), linetype = "dashed", colour = "red") +
  geom_hline(yintercept = log10(1/6), linetype = "dashed", colour = "blue") +
  xlab("Sample size (number of participants)")
```



```{r}
sims_coef_all %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = converged, y = statistic)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = 2, linetype = "dashed")
```


```{r}
sims_coef_all %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = converged, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = fixef_means[4], linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "blue") +
  ylab("Estimate")
```


Estimates of the random by-subject variation for critical effect:

```{r}
sims_coef_all %>%
  filter(grepl("sd_mv_wt\\.subject", term)) %>%
  ggplot(aes(x = sim_type, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(converged ~ Nsubj) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[4]), linetype = "dashed") +
  ylab("Estimate")
```

```{r}
sims_coef_all %>%
  filter(grepl("sd_\\(Intercept\\).subject", term)) %>%
  ggplot(aes(x = converged, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = myalpha) +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[1]), linetype = "dashed") +
  ylab("Estimate")
```





Session info
============

```{r}
sessionInfo()
```
