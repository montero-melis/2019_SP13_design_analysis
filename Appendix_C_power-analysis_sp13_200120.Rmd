---
title: "Appendix C: Power analysis for SP13 replication"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

**[ADD: BY-TRIAL DATA; ITEM VARIANCE ESTIMATE FROM PILOT]**

This knitr script executes and reports a power analysis for our replication of
Shebani and Pulvermüller (2013, *Cortex*), henceforth SP13.

Data simulations are based on our re-analysis of the original data as shared by
SP13. Since that data set did not consist of trial-level observations, we do not
have a way to estimate item variability from it. We assume a generative model
for the simulations with subjects as the only random effects factor, including
both by-subject random intercepts and slopes for all the fixed effects (see 
model specification below).

We adopt the following approach for our power analysis, which really is a type
of Bayes factor design analysis (see Schönbrodt and Wagenmakers, 2017, 
*Psychon Bull Rev*):

1. Simulate random data from the statistical model fitted to the original data.
2. Fit two logisic mixed models to the data using `lme4`, one that includes the
   interaction of interest and another that does not.
3. Compute a Bayes factor for the two models based on the Bayesian information
   criterion (BIC) (see 
   [this link](https://rpubs.com/lindeloev/bayes_factors) for the exact approach).
4. Repeat steps 1-3 a large number of times for different sample sizes.

This document complements our manuscript submitted as a Registered Report to
*Cortex*. See end of document for session info, including package versions, etc.
and the manuscript for further context.


Justification of BIC approach
-----------------------------

There is a practical reason we do not run simulations using the fully Bayesian
approach we will use for our main analysis:
It takes roughly 2 hours to run each analysis of a simulated data set using
`brms` and bridge sampling (as implemented in our analysis pipeline, see
[Appendix_E_analysis_pipeline](https://github.com/montero-melis/2019_SP13-replication_reg-report/blob/master/Appendix_E_analysis_pipeline.html)).
If we used this approach to simulate 1000 data sets for six different Ns, it
would take approximately 
$6 \times 1000 \times 2 = 12000$ hours
or a little more than *one year and four months* to run the simulations.
Our approach is a compromise that allows for a good enough estimation of Bayes
factors taking into account the specifics of our design as opposed to 
off-the-shelve R packages like `BFDA` (Schönbrodt & Stefan, 2018).


Setup workspace
===============

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library("knitr")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("broom")
library("boot")       # for inv.logit()
library("lme4")
library("brms")
library("tictoc")
```


```{r}
# Load original data from SP13
d <- read_csv("data/SP13_orig-data_total-errors_long-format.csv") %>%
  # keep critical interference conditions only
  filter(movement %in% c("arm_paradi", "leg_paradi"))
# For each experimental cell, the maximum number of errors is 48 (see
# "Appendix_B_reanalysis_original.Rmd" for justification0)
d$n <- 48
```


Fit model to original data using `brms`
==========================

Contrast coding for factors:

```{r}
# Use numeric coding to fit models without correlation terms from REs
d$mv <- scale(as.numeric(factor(d$movement)), scale = FALSE) * -2
d$wt <- scale(as.numeric(factor(d$word_type)), scale = FALSE) * -2
d$mv_wt <- d$mv * d$wt
d$subject <- factor(d$subject)
```


`brms` fit
----------

```{r}
# Load from disk - list containing 1) full model, 2) null model, 3) BF
bfm_max <- read_rds("sims_etc/sp13_bfm_max.rds")
```

```{r}
bfm_full <- bfm_max[[1]]
# summary(bfm_full)
```


Bayes Factor for H1 vs H0:

```{r}
bfm_max[[3]]
```



`lme4` fit
----------

Fit the equivalent `lme4` model:

```{r}
fm_max_full <- glmer(
  cbind(errors, n - errors) ~ 1 + mv + wt + mv_wt + (1 + mv + wt + mv_wt | subject),
  data = d, 
  family = "binomial"
  )
fm_max_null <- update(fm_max_full, formula = ~ . - mv_wt)
summary(fm_max_full)
```


BF based on BIC:

```{r}
exp( (BIC(fm_max_null) - BIC(fm_max_full) ) / 2)
```



Estimates for item-level random variability
-------------------------------

[TAKEN FROM THE PILOT DATA...]



Power simulations
=================

Prepare data generating process
-------------------------------

### Load function

Load the **function** that generates binomial data aggregated by subject
([this one](https://github.com/montero-melis/2019_SP13_design_analysis/blob/master/generate_data_fnc.R)).

```{r}
source("generate_data_fnc.R")
```


### Set up simulation parameters

The output from the model fit to the original data serves as input to the function.
We need to put them into the right format:

```{r}
# The info we need is in the summary of fixed effects of the model:
fixef(bfm_full)

# Vector of coefficient means
fixef_means <- fixef(bfm_full)[, 1]
fixef_means

# Covariance matrix (Sigma); we need to square the SEs to convert them to Variances
fixef_sigma <- diag(fixef(bfm_full)[, 2] ^ 2)  # we assume uncorrelated diagonal matrix
fixef_sigma

# Random effects for the model:
VarCorr(bfm_full)$subject$sd
# Specifically we extract by-subject random SDs from this matrix (they are stored
# as a "stddev" attribute of the matrix) and square them to obtain variances:
ranef_sigma_subj <- diag(VarCorr(bfm_full)$subject$sd[, 1]) ^ 2
ranef_sigma_subj
```

An estimate for random effects by item we take from our pilot data (see
[here](https://github.com/montero-melis/2018_replication_sheb-pulv2013/blob/master/1806_pilot_analysis/item-variance_estimates.R)).


```{r}
# We don't have estimates for by-item random slope for movement; we will assume
# it to be the same as the by-subject random variability for the critical
# interaction, which probably is an overestimate but should make our power
# estimates more conservative
ranef_sigma_item <- diag(c(0.65, 0.06)) ^ 2  # square to obtain variances
ranef_sigma_item
```


### Example simulation

We can now call the `simulate_binom` function to generate data:

```{r, message = FALSE}
sim_ex <- simulate_binom(
  Nsubj = 10,
  Nitem = 20,
  fixef_means = fixef_means,
  fixef_sigma = fixef_sigma,
  ranef_sigma_subj = ranef_sigma_subj,
  ranef_sigma_item = ranef_sigma_item
  )
sim_ex
```

If we set `full_output = TRUE`, the output is a list that contains additional
information about the fixed and random effects in the simulation:


```{r, message = FALSE}
simulate_binom(
  Nsubj = 2,
  Nitem = 8,
  fixef_means = fixef_means,
  fixef_sigma = fixef_sigma,
  ranef_sigma_subj = ranef_sigma_subj,
  ranef_sigma_item = ranef_sigma_item,
  full_output = TRUE
  )
```


Functions to analyze simulations
--------------------------------

### Basic function

```{r}
analyze_simulation <- function(df, print_model_summaries = FALSE) {

  # start counter
  tic()

  # Contrast coding (umeric coding allows for models without RE correlations)
  df$mv <- scale(as.numeric(df$movement), scale = FALSE) * -2
  df$wt <- scale(as.numeric(df$word_type), scale = FALSE) * -2
  df$mv_wt <- df$mv * df$wt

  # function to catch convergence issues
  converge <- function (fm) {
  # Paste in case there are several messages
  message <- paste(summary(fm)$optinfo$conv$lme4$messages, collapse = " / ")
  if( is.null(message) ) { "" } else { message }
  }
  
  # Function to fit a full and null model given model formula as string:
  fit_formula <- function (f_str, d = df) {
    formula <- as.formula(f_str)  # convert to formula
    full <- glmer(formula, data = d,
                  control = glmerControl(optCtrl=list(maxfun=1e6)),
                  family = "binomial"
                  )
    null <- update(full, formula = ~ . - mv_wt)

    if (print_model_summaries) {
      print(summary(full))
      print(summary(null))
    }
    list(full, null)
  }
  
  # Fit models without RE correlations using double-bar syntax:
  myf <- "Error ~ mv + wt + mv_wt + (1 + mv + wt + mv_wt || subject) + (1 + mv || item)"
  models <- fit_formula(myf)

  # Put fixed effect model coefficients into one data frame
  model_info <- bind_rows(
    unnest(tidy(models[[1]])) %>%
      mutate(model = "full", convergence = converge(models[[1]])),
    unnest(tidy(models[[2]])%>% filter(group == "fixed" | grepl("mv_wt", term))) %>%
      mutate(model = "null", convergence = converge(models[[2]]))
  ) %>%
    # Now add more concise info (redundantly repeated across rows, can be filtered
    # out later):
    mutate(
      BIC_full = BIC(models[[1]]),
      BIC_null = BIC(models[[2]]),
      BF_BIC = exp( (BIC(models[[2]]) - BIC(models[[1]]) ) / 2),
      conv_full = converge(models[[1]]),
      conv_null = converge(models[[2]]),
      formula_full = myf
    ) %>%
    select(-group)
  # Add the time it took
  mytoc <- toc()
  model_info$t_ellapsed <- mytoc$toc - mytoc$tic
  model_info
}
```


### Example

The output is a one-row data frame with information about the analysis:

```{r, message=FALSE, warning=FALSE}
sim_ex <-  simulate_binom(
  Nsubj = 20,
  Nitem = 10,
  fixef_means = fixef_means,
  fixef_sigma = fixef_sigma,
  ranef_sigma_subj = ranef_sigma_subj,
  ranef_sigma_item = ranef_sigma_item,
  )
# tictoc::tic()
# an_ex <- analyze_simulation(sim_ex, print_model_summaries = TRUE)
# tictoc::toc()
# an_ex
```


If called with `print_model_summaries = TRUE` it also prints the model summaries
to screen.

```{r, message=FALSE, warning=FALSE}
# analyze_simulation(sim_ex, print_model_summaries = TRUE)
```


### Function to analyze many data sets

```{r}
# Function to analyze many data sets, called with pmap
sim_many <- function(rseed, sim_id, Nsubj, ...) {
  set.seed(rseed)
  df <- simulate_binom(Nsubj, ...)
  result <- analyze_simulation(df)
  result$sim_id <- sim_id
  result$rseed <- rseed
  result$Nsubj <- Nsubj
  write_csv(
    result,
    path = "sims_etc/power_simulation_results_append.csv",
    # col_names = TRUE,  # only first time it's run
    append = TRUE)
  result
}
```



### Run the simulations (generate and analyze data sets)

Create a data frame of simulation parameters later called with `purrr::pmap`:

```{r}
# Number of simulations per N:
nb_sims <- 5
# How many different Ns?
params <- tibble(Nsubj = c(15, 60, 96))
params <- params[rep(1 : nrow(params), each = nb_sims), ]
params$rseed <- sample.int(10 ^ 9, size = nrow(params))
params$sim_id <- seq_len(nrow(params))
head(params, 4)
tail(params, 4)
```

Run the simulations:

```{r, message=FALSE, warning=FALSE}
# tictoc::tic()
# mysims <- pmap(
#   .l = params,
#   .f = sim_many,
#   # fixed simulation parameters (for clarity, comment out the ones passed through params)
#   # Nsubj =,
#   Nitem = 104,
#   fixef_means = fixef_means,
#   fixef_sigma = fixef_sigma,
#   ranef_sigma_subj = ranef_sigma_subj,
#   ranef_sigma_item = ranef_sigma_item
#   ) %>%
#   bind_rows()
# tictoc::toc()
```

Keep running until there are enough converged models:

```{r}
sim_till_aim <- function(aim, filter_out = 0) {
  neg2zero <- function(x) ifelse(x < 0, 0, x)
  # retrieve simulations and check which ones converged (both null and full)
  sims <- read_csv("sims_etc/power_simulation_results_append.csv") %>%
    select(rseed, Nsubj, conv_full, conv_null) %>% unique() %>%
    mutate(converged = ( is.na(conv_full) & is.na(conv_null) )) %>%
    group_by(Nsubj) %>%
    summarise(sims_run = n(), converged = sum(converged)) %>%
    mutate(
      Nsubj = as.numeric(Nsubj),
      aim = aim,
      needed_neg = aim - converged,
      needed = neg2zero(needed_neg)) %>%
    # Estimated convergence rate at different sample sizes
    left_join(tibble(Nsubj = c(15, 60, 96), conv_rate = c(0.11, 0.36, 0.51))) %>%
    mutate(nsims = round(needed / conv_rate)) %>%
    filter( (! Nsubj %in% filter_out))
  sims
}
```

Example:

```{r, message=FALSE}
# Estimated nnumber of simulations needed to get to an aim
sim_till_aim(1000)
# Filter out simulations with sample size 15 and 60
sim_till_aim(1000, c(15, 60))
```


```{r}
# Function that checks how many converged simulations there are and runs new
# ones to reach the aim:
run_till_aim <- function(aim, ...) {
  tictoc::tic()
  sims <- sim_till_aim(aim, ...)
  params <- tibble(
    rseed = sample.int(10 ^ 9, size = sum(sims$nsims)),
    Nsubj = rep(sims$Nsubj, sims$nsims)
  )
  params$sim_id <- seq_len(nrow(params))
  params
  pmap(
    .l = params,
    .f = sim_many,
    # fixed simulation parameters (for clarity, comment out the ones passed through params)
    # Nsubj =,
    Nitem = 104,
    fixef_means = fixef_means,
    fixef_sigma = fixef_sigma,
    ranef_sigma_subj = ranef_sigma_subj,
    ranef_sigma_item = ranef_sigma_item
  )
  tictoc::toc()
  }
```


Check and run!

```{r, message=FALSE}
sim_till_aim(750)
sim_till_aim(1000, filter_out = c(15))
# run_till_aim(750, filter_out = c(15))
```




### Load from disk those already run

Load results from disk if already run and show output of simulations:

```{r}
# This version of the file keeps stacking the result of simulations:
mysims <- read_csv("sims_etc/power_simulation_results_append.csv")
# Rearrange columns
mysims <- mysims %>%
  select(sim_id : Nsubj, t_ellapsed, term : conv_null)
head(mysims)
tail(mysims) %>% kable(digits = 2)
```





Analyze results
---------------

### Shape of data frame with results

The data frame (tibble) contains a lot of information about the model fitting.
Mainly it consists of two sets of output:

1) The BIC of the models and corresponding BF_BIC:

```{r}
sims_BIC <- mysims %>%
  select(sim_id : t_ellapsed, BIC_full : conv_null) %>%
  unique() %>%
  mutate(
    converged = is.na(conv_full) & is.na(conv_null)
    )
sims_BIC %>% tail() %>% kable(digits = 2)
```

2) The fixed effect coefficient estimates of each model (in long format):

```{r}
sims_coef <- mysims %>%
  select(sim_id : convergence) %>%
  unique() %>%
  mutate(converged = is.na(convergence) | convergence == "")
sims_coef %>% head() %>% kable(digits = 2)
```


### Basics

Any repeated random seeds?

```{r}
nrow(sims_BIC) - length(unique(sims_BIC$rseed))
```


Number of simulations:

```{r}
with(sims_BIC, addmargins(table(Nsubj, converged, sim_type)))
```


Percentage where both full and null model converged:

```{r}
sims_BIC %>%
  group_by(Nsubj, sim_type) %>%
  summarise(run = n(), converged = sum(converged)) %>%
  mutate(prop_converged = converged / run) %>%
  kable(digits = 2)
```



### BF_BIC

Summary of results

```{r}
by(sims_BIC$BF_BIC, with(sims_BIC, list(Nsubj, sim_type, converged)), summary)
```

Power:

```{r}
sims_BIC %>%
  mutate(BF6 = BF_BIC >= 6) %>%
  group_by(Nsubj, converged, sim_type) %>%
  summarise(power = mean(BF6)) %>%
  ggplot(aes(x = converged, y = power)) +
  geom_bar(stat = "identity") +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = .9)
```


```{r}
sims_BIC %>%
  mutate(BF6 = BF_BIC >= 6) %>%
  group_by(converged, sim_type, Nsubj) %>%
  summarise(power_percent = 100 * mean(BF6)) %>%
  kable(digits = 1)
```


```{r}
ggplot(sims_BIC, aes(x = converged, y = log10(BF_BIC))) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = .15) +
  facet_grid(sim_type ~ Nsubj) +
  geom_hline(yintercept = log10(6), linetype = "dashed") +
  xlab("Both models converged")
```

```{r}
ggplot(sims_BIC %>% filter(converged == TRUE), aes(x = log10(BF_BIC))) + 
  geom_density() +
  facet_grid(sim_type ~ Nsubj)
```


### Significance of critical coefficients

Summary of z-values for critical interaction (in full models):

```{r}
sims_coef %>%
  filter(term == "mv_wt") %>%
  select(statistic) %>%
  pull %>%
  summary
```


```{r}
sims_coef %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = sim_type, y = statistic)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = .15) +
  facet_grid(converged ~ Nsubj) +
  geom_hline(yintercept = 2, linetype = "dashed") +
  xlab("Model converged")
```


Estimates of the critical interaction:

```{r}
sims_coef %>%
  filter(term == "mv_wt") %>%
  ggplot(aes(x = sim_type, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = .15) +
  facet_grid(converged ~ Nsubj) +
  geom_hline(yintercept = fixef_means[4], linetype = "dashed") +
  ylab("Estimate") +
  xlab("Model converged")
```


Estimates of the random by-subject variation for critical effect:

```{r}
sims_coef %>%
  filter(term == "sd_mv_wt.subject") %>%
  ggplot(aes(x = converged, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = .15) +
  facet_grid(. ~ Nsubj) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[4]), linetype = "dashed") +
  ylab("Estimate") +
  xlab("Model converged")
```

```{r}
sims_coef %>%
  filter(term == "sd_(Intercept).subject.3") %>%
  ggplot(aes(x = converged, y = estimate)) + 
  geom_boxplot() +
  geom_jitter(height = 0, alpha = .15) +
  facet_grid(. ~ Nsubj) +
  geom_hline(yintercept = sqrt(diag(ranef_sigma_subj)[1]), linetype = "dashed") +
  ylab("Estimate") +
  xlab("Model converged")
```




Comparison of methods -- why are BF_BICs so high?
================================================

BFs based on BICs are humongous. Is it a consequence of the specific method
to compute these BFs?


Approach
--------

Let's take one generated data set and analyze it using three methods:

1. Intercept only `lme4` model
2. Maximal REs `lme4` model
3. Maximal REs `brms` model and bridge sampling to obtain BFs (as we will do in
our actual data set)

The data set
-----------

Take a data set that doesn't fail to converge for N=100 and is around the median:

```{r}
# sims_BIC %>%
#   filter(converged == TRUE & Nsubj == 96) %>%
#   arrange(BF_BIC) %>%
#   kable
```

We take simulation 29 with a BF of around a billion.

```{r}
# set.seed(158912513)
# df_sim <- simulate_binom(
#   Nsubj = 100,
#   Nitem = 104,
#   fixef_means = fixef_means,
#   fixef_sigma = fixef_sigma,
#   ranef_sigma_subj = ranef_sigma_subj,
#   ranef_sigma_item = ranef_sigma_item
#   )
# df_sim
```


Adjust coding scheme:

```{r}
# # Get the coding scheme right:
# contrasts(df_sim$movement)  <- contr.sum(2)
# contrasts(df_sim$movement)
# contrasts(df_sim$word_type) <- contr.sum(2)
# contrasts(df_sim$word_type)
```




Analyze with 3 methods
----------------------

### Intercepts only lme4

```{r}
# fm_interc_full <-  glmer(
#   Error ~ movement * word_type +
#     # (1 + movement * word_type | subject) +
#     # (1 + movement | item),
#     (1 | subject) +
#     (1 | item),
#   data = df_sim, family = "binomial"
# )
# # Without fixef interaction term:
# fm_interc_null <- update(fm_interc_full, formula = ~ . - movement : word_type)
```

BICs and BIC-based BF:

```{r}
# BIC(fm_interc_full)
# BIC(fm_interc_null)
# # BF
# exp( (BIC(fm_interc_null) - BIC(fm_interc_full) ) / 2)
```

Model summaries

```{r}
# summary(fm_interc_null)
# summary(fm_interc_full)
```


### Maximal lme4

```{r}
# tic()
# fm_max_full <-  glmer(
#   Error ~ movement * word_type +
#     (1 + movement * word_type | subject) +
#     (1 + movement | item),
#     # (1 | subject) +
#     # (1 | item),
#   data = df_sim, family = "binomial"
# )
# toc()
# tic()
# # Without fixef interaction term:
# fm_max_null <- update(fm_max_full, formula = ~ . - movement : word_type)
# toc()
```

BICs and BIC-based BF:

```{r}
# BIC(fm_max_full)
# BIC(fm_max_null)
# # BF
# exp( (BIC(fm_max_null) - BIC(fm_max_full) ) / 2)
```

Model summaries

```{r}
# summary(fm_max_null)
# summary(fm_max_full)
```


### Maximal with brms

```{r}
# myprior <- set_prior("normal(0, 2)", class = "b")
# print(myprior)
# # Model *without* interaction [took 3.3h]
# tic()
# bfm_null <- brm(
#   Error ~
#     1 + movement + word_type +  # critical manipulations but no interaction
#     (1 + movement * word_type | subject) + (1 + movement | item),  # maximal random structure
#   data = df_sim,
#   prior = myprior,
#   family = "bernoulli",
#   iter = 15000, warmup = 2000, chains = 4,  # https://discourse.mc-stan.org/t/bayes-factor-using-brms/4469/3
#   save_all_pars = TRUE  # necessary for brms::bayes_factor() later
# )
# toc()
# # fit full model (with interaction):
# tic()
# bfm_full <- brm(
#   Error ~
#     1 + movement * word_type +  # critical manipulations and interaction
#     (1 + movement * word_type | subject) + (1 + movement | item),  # maximal random structure
#   data = df_sim,
#   prior = myprior ,
#   family = "bernoulli",
#   iter = 15000, warmup = 2000, chains = 4,  # https://discourse.mc-stan.org/t/bayes-factor-using-brms/4469/3
#   save_all_pars = TRUE  # necessary for brms::bayes_factor() later
# )
# toc()
```

```{r}
# summary(bfm_null)
# summary(bfm_full)
```


BF using bridge sampling:

```{r}
# BF_bfm <- brms::bayes_factor(bfm_full, bfm_null)
# BF_bfm
```


### Save it all to disk

```{r}
# model_list <- list(
#   fm_interc_null, fm_interc_full, fm_max_null, fm_max_full, 
#   bfm_null, bfm_null, BF_bfm
#   )
# # write_rds(model_list, path = "sims_etc/model_compare_158912513.Rda")
```


Compare the three methods
------------------------

```{r}
# model_list1 <- read_rds("sims_etc/model_compare_158912513.Rda")
# model_list2 <- read_rds("sims_etc/model_compare_189941429.Rda")
```


Function

```{r}
# compare_models_BIC <- function (model_list) {
# 
#   p <- function(x) print(x)
# 
#   p("fm_interc_null")
#   p(BIC(model_list[[1]]))
#   p("fm_interc_full")
#   p(BIC(model_list[[2]]))
#   p("BIC_BF")
#   p(exp( (BIC(model_list[[1]]) - BIC(model_list[[2]]) ) / 2))
#   cat("\n")
# 
#   p("fm_max_null")
#   p(BIC(model_list[[3]]))
#   p("fm_max_full")
#   p(BIC(model_list[[4]]))
#   p("BIC_BF")
#   p(exp( (BIC(model_list[[3]]) - BIC(model_list[[4]]) ) / 2))
#   cat("\n")
# 
#   p("BF bridge sampling with brms model")
#   p(model_list[[7]])
# }
# 
# model_summaries <- function(model_list) {
#   p <- function(x) print(x)
#   p(summary(model_list[[2]]))
#   p(summary(model_list[[3]]))
#   p(summary(model_list[[4]]))
#   p(summary(model_list[[6]]))
# }
```




```{r}
# compare_models_BIC(model_list1)
# compare_models_BIC(model_list2)
# 
# model_summaries(model_list1)
# model_summaries(model_list2)
```


Conclusion (Revise this!!!)
==========

Our proposed design with $N_{min}=60$ and $N_{max}=96$ (step size = 12) and the
evidence threshold set at $BF_{10} > 6$ or $BF_{01} > 6$ will largely suffice to
guarantee high power (>90%) if the effect was as strong as reported in the
original study. 
Taking into account, however, that a) our re-analysis of the original data suggests
that the effect size might have been severely over-estimated, and b) the
original ANOVA-based analysis seem inappropriate for the data at hand, we have
chosen a design that will also guarantee  a high probability of compelling
evidence using an improved analysis method and with a small-to-medium effect size.


Session info
============

```{r}
sessionInfo()
```
