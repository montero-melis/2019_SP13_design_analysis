---
title: "BF design analysis for SP13 replication"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

This knitr script executes and reports the Bayes factor design analyses (BFDA) 
for our replication of Shebani and Pulvermüller (2013, *Cortex*), henceforth SP13.
All simulations are done in the framework laid out in Schönbrodt and Wagenmakers
(2017, *Psychon Bull Rev*), and implemented using the `BFDA` package (Schönbrodt
& Stefan, 2018; https://github.com/nicebread/BFDA).
See end of document for session info, including package versions, etc.

This document complements our manuscript submitted as a Registered Report to
*Cortex*. See the manuscript for further context.


Setup workspace
===============

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library("knitr")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("boot")       # for inv.logit()
library("BFDA")
library("brms")
```


Load model fitted on original data:

```{r, message=FALSE}
# Binomial GLMM of the original data (fitted with brms); see script 
# "reanalysis_original.Rmd"
bfm_binom <- readRDS("sims_etc/bayes_glmm_default.rds")
```

*brms* has a `posterior_samples(x, pars)` function to get posterior traces for
specific coefficients. We pass our fitted *brms* model as x, and the predictor
of interest as pars, and obtain a dataframe with the samples in return.
This posterior from the original we can then use as the prior for the Bayes 
factor design analysis (BFDA) in our replication.

```{r}
# Extract posterior estimates for the critical interaction effect from the brms
# model (pull converts it to a numerical vector):
beta_posterior <- pull(posterior_samples(bfm_binom, "movementarm_vs_leg:word_typearm_vs_leg"))
```


Convenience function:

```{r}
# Wrapper function to display the results of the simulation:
results_sim <- function(sim, pow = FALSE, bound = c(1/6, 6)) {
  print(BFDA.analyze(sim, design = "sequential", boundary = c(1/6, 6)))
  plot(sim, boundary = bound)  # pass argument if it crashes with default
  if (pow) { SSD(sim, power=.9, boundary=c(1/6, 6)) }
}
```


Some notes on the simulations below
===================================

1. For each set of simulations below, we simulated 10 000 studies (`B = 10000`),
which is the recommendation by the package authors (see 
[BFDA manual](https://rawgit.com/nicebread/BFDA/master/package/doc/BFDA_manual.html))
2. Running these simulations often takes a long time (from a few minutes to a few
hours on a laptop). Here and below we have run the simulations once and saved
them to disk so that they can simply be loaded when running this script. To run
them anew, please uncomment the relevant code snippets.


BFDA using effect size and analysis reported in the original study
============================================

SP13 report an effect size of Cohen's $d = 1.25$ (p.226) for the critical
interaction effect obtained from a within-subjects ANOVA. Our first BFDA
follows this estimate, applying a paired t-test design, which is the closest
available test to the within-subjects ANOVA, and also the one that naturally
accepts Cohen's $d$ as an effect size estimate.

We run the simulation with the default (non-informative) prior so as to err on
the conservartive side.(see Stefan et al., 2019, *Behav Res Meth*, for a 
discussion of choice of prior).



Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations and save to disk:
# 
# # Under alternative hypothesis H1 with ES d = 1.25
# sim_H1_d_125_defPrior <- BFDA.sim(
#   expected.ES = 1.25,  # as reported in SP13
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_125_defPrior, "sims_etc/sim_H1_d_125_defPrior.rds")
# 
# # Under null hypothesis H0
# sim_H0_d_125_defPrior <- BFDA.sim(
#   expected.ES = 0,  # under H0
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_d_125_defPrior, "sims_etc/sim_H0_d_125_defPrior.rds")
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_125_defPrior <- read_rds("sims_etc/sim_H1_d_125_defPrior.rds")
sim_H1_d_125_defPrior

sim_H0_d_125_defPrior <- read_rds("sims_etc/sim_H0_d_125_defPrior.rds")
sim_H0_d_125_defPrior
```


Results under H1
----------------

```{r, warning=FALSE}
results_sim(sim_H1_d_125_defPrior, T)
```

Results under H0
----------------

```{r}
results_sim(sim_H0_d_125_defPrior)
```



BFDA based on Cohen's *d* from t-test on the original data
======================================================

When we re-analyze the data for the critical interaction using a paired t-test,
we obtain an effect size of Cohen's $d = 0.9$ (see `reanalysis_original.html`).

Next we run simulations based on this effect size, again using a paired t-test.



Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations and save to disk (took ~1h):
# 
# # Under alternative hypothesis H1 with ES d = 1.25
# sim_H1_d_09_defPrior <- BFDA.sim(
#   expected.ES = 0.9,  # from our t-test reanalysis
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_09_defPrior, "sims_etc/sim_H1_d_09_defPrior.rds")
# 
# # Under null hypothesis H0: The simulation here is the same as in
# # sim_H0_d_125_defPrior, so no need to run it anew
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_09_defPrior <- read_rds("sims_etc/sim_H1_d_09_defPrior.rds")
sim_H1_d_09_defPrior
```

## Under H1

```{r, warning=FALSE}
results_sim(sim_H1_d_09_defPrior, T)
```


## Under H0

Same as above.




BFDA based on ANOVA as in original study and $d$ converted from $\eta^2$
=======================================================================

We are not able to reproduce the effect size reported in SP13 (see 
`reanalysis_original.html`).
When we re-analyze the data with an ANOVA (as in the original) and convert the
$\eta^2$ of the interaction to Cohen's $d$, we obtain an effect size of Cohen's
$d = 0.41$ (see `reanalysis_original.html`).

We therefoew next run simulations based on this effect size ($d = 0.4$), again
using a paired t-test.


Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations under alternative hypothesis H1 (took ~40'):
# sim_H1_d_04_defPrior <- BFDA.sim(
#   expected.ES = 0.4,  # from eta-squared in ANOVA
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_04_defPrior, "sims_etc/sim_H1_d_04_defPrior.rds")
# 
# # Under null hypothesis H0: The simulation here is the same as in
# # sim_H0_d_125_defPrior, so no need to run it anew
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_04_defPrior <- read_rds("sims_etc/sim_H1_d_04_defPrior.rds")
sim_H1_d_04_defPrior
```

## Under H1

```{r, warning=FALSE}
results_sim(sim_H1_d_04_defPrior, FALSE)
```


## Under H0

Same as above.








BFDA based on our improved re-analysis (binomial GLMM) of the original data
=======================================================================

Analyzing the error data using ANOVAs (or t-tests), as the authors did,
violates some of the assumptions of those models.
A better statistical analysis is to treat the errors participants made as
arising from a binomial distribution. In order to additionally capture 
subject-level variability, we used Generalized Linear Mixed Models (GLMMs).


Re-analysis of the original data with a binomial GLMM
----------------------------------------------------

In our re-analysis of the data (see `reanalysis_original.Rmd`), we used a
Bayesian version of the binomial GLMM using the R package *brms*. The 
coefficients in such a model estimate effects in log-odds. The critical
interaction of movement condition (hand vs leg movement) and word type 
(hand-related vs leg-related verb) also comes out as highly significant. 


### Model summary

This is the output of this model:

```{r}
summary(bfm_binom)
```



### Presence of an interaction effect

*brms* allows us to test the hypothesis that the crucial interaction is larger
than zero (i.e., that there is an interference effect):

```{r}
hypothesis(bfm_binom, "movementarm_vs_leg:word_typearm_vs_leg > 0")
```

The evidence ratio for the presence of an interference effect is overwhelming!


### Effect size estimate

However, in terms of effect size, the odds of making an error if effector and
word type are the same (arm movement and arm word or leg movements and leg words),
as opposed to when the two differ, is
$e ^ {`r round(fixef(bfm_binom)[4], 2)`} =`r round(exp(fixef(bfm_binom)[4]), 2)`$.
This is the *interference effect* of interest!

**An effect of `r round(fixef(bfm_binom)[4], 2)` log-odds is *very* small**.
For orientation, a common rule of thumb to interpret effect sizes in log-odds is
as follows (see Chen et al., 2010):

- Small effect  = 0.52 log-odds (= 1.68 OR)
- Medium effect = 1.24 log-odds (= 3.47 OR)
- Large effect  = 1.90 log-odds (= 6.71 OR)

In contrast to the effect size reported by SP13 (Cohen's $d = 1.25$), which
was substantially larger than "large", this re-analysis suggests that the
effect was substantially smaller than "small".


BFDA with effect size as estimated from our improved re-analysis of the data
--------------------------------------

If we take the output from our improved binomial GLMM re-analysis of the data as
the prior for the BFDA, the estimated sample size we would need is huge, as 
shown next.


### Simulate hypothetical studies

In this simulation,

- We simulate a sequential design with minN = 100, maxN = 3000 and a step
size of 100 participants.
- We use as the effect size estimate (passed to the `expected.ES` argument)
the posterior draw for the interaction effect from the Bayesian GLMM model;
- We use a default (non-informative) prior as the analysis prior;
- We run a AB-test (instead of a t-test), which is the appropriate test for
effects measured on a (log-)odds ratio scale.


Simulate data under H1 and H0 and save to disk.

```{r}
# # Under alternative hypothesis H1 (~2.7h)
# sim_H1_origES_defPrior <- BFDA.sim(
#   expected.ES = beta_post,  # posterior of original becomes prior
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=100, n.max=3000, stepsize = 100, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_origES_defPrior, "sims_etc/sim_H1_origES_defPrior.rds")
# 
# # Under null hypothesis H0 (~2.5h)
# sim_H0_origES_defPrior <- BFDA.sim(
#   expected.ES = 0,  # no effect
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=100, n.max=3000, stepsize = 100, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_origES_defPrior, "sims_etc/sim_H0_origES_defPrior.rds")
```

Load from disk

```{r}
# Load from disk
sim_H1_origES_defPrior <- read_rds("sims_etc/sim_H1_origES_defPrior.rds")
sim_H1_origES_defPrior

sim_H0_origES_defPrior <- read_rds("sims_etc/sim_H0_origES_defPrior.rds")
sim_H0_origES_defPrior
```

### Under H1


```{r, warning=FALSE}
results_sim(sim_H1_origES_defPrior, T)
```


### Under H0

```{r}
results_sim(sim_H0_origES_defPrior)
```

These simulations indicate that not even an N as large as 3000 (!) participants
would suffice to adhere to the journal guidelines to "guarantee data collection
until the Bayes factor is at least 6 times in favour of the experimental
hypothesis over the null hypothesis (or vice versa)."



BFDA based on improved analysis method and small-to-medium effect
=================================================================

Since adjusting our sample size to the estimates above is unfeasible, we opted
for a design that would allow us to plan for compelling evidence if the effect
size was either "medium" (i.e. 1.25 log-odds) or "small-to-medium" (i.e., 0.9
log-odds).
As previously, in these simulations we don't commit to any strong prior for the
Bayes factor analysis.

We follow Chen et al.'s (2010) rules of thumb regarding what counts as "small",
"medium", and "large" effect sizes measured in log-odds (who in turn derive their
estimates from Cohen's [1988] guidelines):


```{r, include = FALSE}
ES_chen <- tibble(
  "Label (Chen et al., 2010)" = c("Small", "Medium", "Large"),
  odds  = c(1.68, 3.47, 6.71)
  ) %>%
  mutate("log-odds" = log(odds))
ES_chen %>% kable(digits = 2)
```

We ran two simulations, one setting the effect size to a "medium" effect
(1.25 log-odds) and one with a "small"-to-"medium" effect size (0.9 log-odds).


We follow Borenstein et al.'s (2009, p.47) conversion formula from Cohen's $d$
to log-odds (see also Hasselblad and Hedges, 1995):

\[
log-odds = d \frac{\pi}{\sqrt{3}}
\]

Applied to Cohen's (1988) rule of thumb, this yields:

```{r, echo=FALSE}
d_2_LO <- tibble(
  "label" = c("Our re-analysis of original data", "Small", "Medium", "Large", "Reported in SP13"),
  d = c(.15 * sqrt(3) / pi, .2, .5, .8, 1.25)
  ) %>%
  mutate("log-odds" = d * pi / sqrt(3))
d_2_LO %>% kable(digits = 2)
```





Simulate hypothetical studies under medium and small-to-medium effect sizes
---------------------------------------------------------------------

```{r}
# # Under alternative hypothesis H1 with medium ES (~20')
# sim_H1_LO_125_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 1.25,
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_LO_125_defPrior_steps12, "sims_etc/sim_H1_LO_125_defPrior_steps12.rds")
# 
# # Under alternative hypothesis H1 with small-to-medium ES (~20')
# sim_H1_LO_09_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 0.9,
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_LO_09_defPrior_steps12, "sims_etc/sim_H1_LO_09_defPrior_steps12.rds")
# 
# # Under alternative hypothesis H1 with medium ES (~40') and maxN = 144
# sim_H1_LO_09_defPrior_steps12_maxN144 <- BFDA.sim(
#   expected.ES = 0.9,
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=144, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_LO_09_defPrior_steps12_maxN144, "sims_etc/sim_H1_LO_09_defPrior_steps12_maxN144.rds")
# 
# 
# # Under null hypothesis H0 (~20')
# sim_H0_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 0,  # no effect
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_defPrior_steps12, "sims_etc/sim_H0_defPrior_steps12.rds")
```

Load from disk

```{r}
# Load from disk
sim_H1_LO_125_defPrior_steps12 <- read_rds("sims_etc/sim_H1_LO_125_defPrior_steps12.rds")
sim_H1_LO_125_defPrior_steps12

sim_H1_LO_09_defPrior_steps12 <- read_rds("sims_etc/sim_H1_LO_09_defPrior_steps12.rds")
sim_H1_LO_09_defPrior_steps12

sim_H1_LO_09_defPrior_steps12_maxN144 <- read_rds("sims_etc/sim_H1_LO_09_defPrior_steps12_maxN144.rds")
sim_H1_LO_09_defPrior_steps12_maxN144

sim_H0_defPrior_steps12 <- read_rds("sims_etc/sim_H0_defPrior_steps12.rds")
sim_H0_defPrior_steps12
```


Under H1 and medium effect size
-------------------------------

```{r, warning=FALSE}
results_sim(sim_H1_LO_125_defPrior_steps12, T)
```


Under H1 and small-to-medium effect size
-------------------------------

```{r, warning=FALSE}
results_sim(sim_H1_LO_09_defPrior_steps12, T)
```


Under H1 and small-to-medium effect size with maxN=144
-------------------------------

```{r, warning=FALSE}
results_sim(sim_H1_LO_09_defPrior_steps12_maxN144, T, bound =  c(1/5, 6))
```


Under H0
--------

```{r}
results_sim(sim_H0_defPrior_steps12)
```


Conclusion
==========

Our proposed design with $N_{min}=60$ and $N_{max}=96$ (step size = 12) and the
evidence threshold set at $BF_{10} > 6$ or $BF_{01} > 6$ will largely suffice to
guarantee high power (>90%) if the effect was as strong as reported in the
original study. However, since a) our re-analysis of the original data suggests
that the effect size might have been severely over-estimated, and b) the
original ANOVA-based analysis seem inappropriate for the data at hand, we have
chosen a design that will also guarantee  a high probability of compelling
evidence using an improved analysis method and with a small-to-medium effect size.


Session info
============

```{r}
sessionInfo()
```
