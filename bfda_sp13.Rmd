---
title: "BF design analysis for SP13 replication"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

This script carries out a number of Bayes factor design analyses (BFDA) based on
the framework laid out in Schönbrodt and Wagenmakers (2017, *Psychon Bull Rev*).
All BFDA apply to our intended replication of Shebani and Pulvermüller 
(2013, *Cortex*), henceforth SP13.

What differs between the BFDAs are the assumptions we make about a) effect size
and b) the type of test we will carry out. See details below.


Setup workspace
===============

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library("knitr")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("boot")       # for inv.logit()
library("BFDA")
library("sjPlot")     # tab_model()
library("brms")
```


Load model fitted on original data and posterior for the interaction effect:

```{r, message=FALSE}
# Binomial GLMM of the original data (fitted with brms); see script 
# "reanalysis_original.Rmd"
bfm_binom <- readRDS("sims_etc/bayes_glmm_default.rds")
# Load posterior estimates for the critical interaction effect from the brms
# model (read from file and convert to vector):
beta_post <- read_csv("sims_etc/beta_posterior.csv") %>%
  pull(1)
```



BFDA using effect size and analysis reported in the original study
============================================

SP13 report an effect size of Cohen's $d = 1.25$ (p.226) for the critical
interaction effect obtained from a within-subjects ANOVA. Our first BFDA
follows this estimate, applying a paired t-test design, which is the closest
available test to the within-subjects ANOVA, and also the one that naturally
accepts Cohen's $d$ as an effect size estimate.

We run the simulation with the default (non-informative) prior so as to err on
the conservartive side.(see Stefan et al., 2019, *Behav Res Meth*, for a 
discussion of choice of prior).



Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations and save to disk:
# 
# # Under alternative hypothesis H1 with ES d = 1.25
# sim_H1_d_125_defPrior <- BFDA.sim(
#   expected.ES = 1.25,  # as reported in SP13
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_125_defPrior, "sims_etc/sim_H1_d_125_defPrior.rds")
# 
# # Under null hypothesis H0
# sim_H0_d_125_defPrior <- BFDA.sim(
#   expected.ES = 0,  # under H0
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_d_125_defPrior, "sims_etc/sim_H0_d_125_defPrior.rds")
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_125_defPrior <- read_rds("sims_etc/sim_H1_d_125_defPrior.rds")
sim_H1_d_125_defPrior

sim_H0_d_125_defPrior <- read_rds("sims_etc/sim_H0_d_125_defPrior.rds")
sim_H0_d_125_defPrior
```


```{r}
# Wrapper function to display the results of the simulation:
results_sim <- function(sim, pow = FALSE) {
  print(BFDA.analyze(sim, design = "sequential", boundary = c(1/6, 6)))
  plot(sim, boundary=c(1/6, 6))
  if (pow) { SSD(sim, power=.89, boundary=c(1/6, 6)) }  # pow=.89 bc there seems to be a small bug
}
```


Results under H1
----------------

```{r, warning=FALSE}
results_sim(sim_H1_d_125_defPrior, T)
```

Results under H0
----------------

```{r}
results_sim(sim_H0_d_125_defPrior)
```



BFDA based on Cohen's *d* from t-test on the original data
======================================================

When we re-analyze the data for the critical interaction using a paired t-test,
we obtain an effect size of Cohen's $d = 0.9$ (see `reanalysis_original.html`).

Next we run simulations based on this effect size, again using a paired t-test.



Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations and save to disk (took ~1h):
# 
# # Under alternative hypothesis H1 with ES d = 1.25
# sim_H1_d_09_defPrior <- BFDA.sim(
#   expected.ES = 0.9,  # from our t-test reanalysis
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_09_defPrior, "sims_etc/sim_H1_d_09_defPrior.rds")
# 
# # Under null hypothesis H0: The simulation here is the same as in
# # sim_H0_d_125_defPrior, so no need to run it anew
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_09_defPrior <- read_rds("sims_etc/sim_H1_d_09_defPrior.rds")
sim_H1_d_09_defPrior
```

## Under H1

```{r, warning=FALSE}
results_sim(sim_H1_d_09_defPrior, T)
```


## Under H0

Same as above.




BFDA based on ANOVA as in original study and $d$ converted from $\eta^2$
=======================================================================

When we re-analyze the data with an ANOVA and convert the $\eta^2$ of the
interaction to Cohen's $d$ (using the online converter REF), we obtain an effect
size of Cohen's $d = 0.41$ (see `reanalysis_original.html`).

Next we run simulations based on this effect size ($d = 0.4$), again using a
paired t-test.


Simulate hypothetical studies
-----------------------------

```{r}
# # Run simulations under alternative hypothesis H1 (took ~40'):
# sim_H1_d_04_defPrior <- BFDA.sim(
#   expected.ES = 0.4,  # from eta-squared in ANOVA
#   type = "t.paired",
#   prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)),  # default non-informative
#   n.min=12, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_d_04_defPrior, "sims_etc/sim_H1_d_04_defPrior.rds")
# 
# # Under null hypothesis H0: The simulation here is the same as in
# # sim_H0_d_125_defPrior, so no need to run it anew
```

Once run, load from disk:

```{r}
# Load from disk
sim_H1_d_04_defPrior <- read_rds("sims_etc/sim_H1_d_04_defPrior.rds")
sim_H1_d_04_defPrior
```

## Under H1

```{r, warning=FALSE}
results_sim(sim_H1_d_04_defPrior, T)
```


## Under H0

Same as above.








BFDA based on our improved re-analysis (binomial GLMM) of the original data
=======================================================================

Analyzing the error data using ANOVAs (or t-tests), as the authors did,
violates some of the assumptions of those models.
A better statistical analysis is to treat the errors participants made as
arising from a binomial distribution. In order to additionally capture 
subject-level variability, we can use Generalized Linear Mixed Models (GLMMs).


Re-analysis of the original data with a binomial GLMM
----------------------------------------------------

In our re-analysis of the data (see `reanalysis_original.Rmd`), we used a
Bayesian version of the binomial GLMM using the R package *brms*. The 
coefficients in such a model estimate effects in log-odds. The critical
interaction of movement condition (hand vs leg movement) and word type 
(hand-related vs leg-related verb) also comes out as highly significant. 


### Model summary

This is the output of this model:

```{r}
summary(bfm_binom)
```

Or with a prettier and somewhat simplified layout (note effects are converted to 
odd-ratios):

```{r}
tab_model(bfm_binom)
```


### Presence of an interaction effect

*brms* allows us to test the hypothesis that the crucial interaction is larger
than zero (i.e., that there is an interference effect):

```{r}
hypothesis(bfm_binom, "movementarm_vs_leg:word_typearm_vs_leg > 0")
```

The evidence ratio for the presence of an interference effect is overwhelming!


### Effect size estimate

However, in terms of effect size, the odds of making an error if effector and
word type are the same (arm movement and arm word or leg movements and leg words),
as opposed to when the two differ, is
$e ^ {`r round(fixef(bfm_binom)[4], 2)`} =`r round(exp(fixef(bfm_binom)[4]), 2)`$.
This is the *interference effect* of interest!

**An effect of `r round(fixef(bfm_binom)[4], 2)` log-odds is *very* small**.
For orientation, a common rule of thumb to interpret effect sizes in log-odds is
as follows (see Chen et al., 2010):

- Small effect  = 0.52 log-odds (= 1.68 OR)
- Medium effect = 1.24 log-odds (= 3.47 OR)
- Large effect  = 1.90 log-odds (= 6.71 OR)

In contrast to the effect size reported by SP13 (Cohen's $d = 1.25$), which
was substantially larger than "large", this re-analysis suggests that the
effect was substantially smaller than "small".


BFDA with effect size as estimated from our improved re-analysis of the data
--------------------------------------

If we take the output from our improved binomial GLMM re-analysis of the data as
the prior for the BFDA, the estimated sample size we would need is huge, as 
shown next.


### Simulate hypothetical studies

In this simulation,

- We simulate a sequential design with minN = 100, maxN = 3000 and a step
size of 100 participants.
- We use as the effect size estimate (passed to the `expected.ES` argument)
the posterior draw for the interaction effect from the Bayesian GLMM model;
- We use a default (non-informative) prior as the analysis prior;
- We run a AB-test (instead of a t-test), which is the appropriate test for
effects measured on a (log-)odds ratio scale.


Simulate data under H1 and H0 and save to disk.

```{r}
# # Under alternative hypothesis H1 (~2.7h)
# sim_H1_origES_defPrior <- BFDA.sim(
#   expected.ES = beta_post,  # posterior of original becomes prior
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=100, n.max=3000, stepsize = 100, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_origES_defPrior, "sims_etc/sim_H1_origES_defPrior.rds")
# 
# # Under null hypothesis H0 (~2.5h)
# sim_H0_origES_defPrior <- BFDA.sim(
#   expected.ES = 0,  # no effect
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=100, n.max=3000, stepsize = 100, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_origES_defPrior, "sims_etc/sim_H0_origES_defPrior.rds")
```

Load from disk

```{r}
# Load from disk
sim_H1_origES_defPrior <- read_rds("sims_etc/sim_H1_origES_defPrior.rds")
sim_H1_origES_defPrior

sim_H0_origES_defPrior <- read_rds("sims_etc/sim_H0_origES_defPrior.rds")
sim_H0_origES_defPrior
```

### Under H1


```{r, warning=FALSE}
results_sim(sim_H1_origES_defPrior, T)
```


### Under H0

```{r}
results_sim(sim_H0_origES_defPrior)
```

These simulations indicate that not even an N as large as 3000 (!) participants
would suffice to adhere to the journal guidelines to "guarantee data collection
until the Bayes factor is at least 6 times in favour of the experimental
hypothesis over the null hypothesis (or vice versa)."



BFDA based on improved analysis method and small-to-medium effect
=================================================================

Since adjusting our sample size to the estimates above is unfeasible, we opted
for a design that would allow us to plan for compelling evidence if the effect
size was either "medium" (i.e. 1.25 log-odds) or "small-to-medium" (i.e., 0.9
log-odds).
As previously, in these simulations we don't commit to any strong prior for the
Bayes factor analysis.

We follow Chen et al.'s (2010) rules of thumb regarding what counts as "small",
"medium", and "large" effect sizes measured in log-odds (who in turn derive their
estimates from Cohen's [1988] guidelines):


```{r, include = FALSE}
ES_chen <- tibble(
  "Label (Chen et al., 2010)" = c("Small", "Medium", "Large"),
  odds  = c(1.68, 3.47, 6.71)
  ) %>%
  mutate("log-odds" = log(odds))
ES_chen %>% kable(digits = 2)
```

We ran two simulations, one setting the effect size to a "medium" effect
(1.25 log-odds) and one with a "small"-to-"medium" effect size (0.9 log-odds).



Simulate hypothetical studies under medium and small-to-medium effect sizes
---------------------------------------------------------------------

```{r}
# # Under alternative hypothesis H1 with medium ES (~20')
# sim_H1_LO_125_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 1.25,
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_LO_125_defPrior_steps12, "sims_etc/sim_H1_LO_125_defPrior_steps12.rds")
# 
# # Under alternative hypothesis H1 with small-to-medium ES (~20')
# sim_H1_LO_09_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 0.9,
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H1_LO_09_defPrior_steps12, "sims_etc/sim_H1_LO_09_defPrior_steps12.rds")
# 
# # Under null hypothesis H0 (~20')
# sim_H0_defPrior_steps12 <- BFDA.sim(
#   expected.ES = 0,  # no effect
#   type = "abtest",
#   prior=list("normal", list(prior.mean = 0, prior.variance = 1)),
#   options.sample = list(effecttype = "logOR"),
#   n.min=60, n.max=96, stepsize = 12, boundary=Inf,
#   alternative="greater",
#   B=10000, verbose=TRUE, cores=1)
# # Save to disk:
# write_rds(sim_H0_defPrior_steps12, "sims_etc/sim_H0_defPrior_steps12.rds")
```

Load from disk

```{r}
# Load from disk
sim_H1_LO_125_defPrior_steps12 <- read_rds("sims_etc/sim_H1_LO_125_defPrior_steps12.rds")
sim_H1_LO_125_defPrior_steps12

sim_H1_LO_09_defPrior_steps12 <- read_rds("sims_etc/sim_H1_LO_09_defPrior_steps12.rds")
sim_H1_LO_09_defPrior_steps12

sim_H0_defPrior_steps12 <- read_rds("sims_etc/sim_H0_defPrior_steps12.rds")
sim_H0_defPrior_steps12
```


Under H1 and medium effect size
-------------------------------

```{r, warning=FALSE}
results_sim(sim_H1_LO_125_defPrior_steps12, T)
```


Under H1 and small-to-medium effect size
-------------------------------

```{r, warning=FALSE}
results_sim(sim_H1_LO_09_defPrior_steps12, T)
```


Under H0
--------

```{r}
results_sim(sim_H0_defPrior_steps12)
```


Conclusion
==========

Our proposed design with $N_{min}=60$ and $N_{max}=96$ (step size = 12) and the
evidence threshold set at $BF_{10} > 6$ or $BF_{01} > 6$ will largely suffice to
guarantee high power (>90%) if the effect was as strong as reported in the
original study. However, since a) our re-analysis of the original data suggests
that the effect size might have been severely over-estimated, and b) the
original ANOVA-based analysis seem inappropriate for the data at hand, we have
chosen a design that will also guarantee  a high probability of compelling
evidence using an improved analysis method and with a small-to-medium effect size.


Session info
============

```{r}
sessionInfo()
```
