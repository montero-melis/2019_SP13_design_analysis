---
title: "Re-analysis of original study (Shebani and Pulvermüller, 2013)"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---

Setup workspace
===============

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("knitr")
library("lme4")
library("brms")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("boot")       # for inv.logit()
```



Original data set
=================


```{r}
# Load data
d_all_conds <- read_csv("orig_study/SP13_orig-data_total-errors_long-format.csv")
```

```{r}
kable(head(d_all_conds))
str(d_all_conds)
```

- The data show the total number of errors, adding all three types of errors:
omissions, replacements, and transpositions.
- This data set contains the data for all four movement conditions:
`r unique(d_all_conds$movement)`. However, we are only interested in
the arm/leg movement conditions, so we subset the data:

```{r}
d <- d_all_conds %>% filter(movement %in% c("arm_paradi", "leg_paradi"))
kable(head(d))
str(d)
```


Plot the data:

```{r}
ggplot(d, aes(x = word_type, y = errors, colour = word_type)) +
  geom_boxplot() +
  facet_grid(. ~ movement)
```

We can qualitatively see the cross-over interaction. Note that although the
variability is large, the effect is still able to come out if there is sufficient
consistency within subjects, as it is a within-subjects design. This can be
better appreciated in the following plot:

```{r}
d %>% mutate(sbj_wtype = paste(subject, word_type, sep = "_")) %>%
  ggplot(aes(x = movement, colour = word_type, y = errors)) +
  geom_jitter(height = 0, width = .05, alpha = .5) +
  geom_line(aes(group = sbj_wtype), alpha = .5) +
  stat_summary(fun.y = mean, geom = "point", size = 4,
               position = position_dodge(width = .2)) +
  stat_summary(fun.y = mean, geom = "line", aes(group = word_type), size = 2,
               position = position_dodge(width = .2)) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 2,
               position = position_dodge(width = .2))
```

The thick dots and lines show mean number of errors and bootstrapped
(non-parametric) 95% CIs.
The thinner dots and lines show subject data. There seems to be quite a lot of
consistency by subjects, i.e. the slopes of the thin lines mostly go in the same
direction as the slope of the thicker lines.


Replicate originally reported analysis with ANOVAs
==================================================


## Data summaries

First, let's see if our data summaries look like theirs reported in Table 2
(SP13, p.226):

```{r}
d_all_conds %>%
  group_by(movement, word_type) %>%
  summarise(
    M  = round(mean(errors), 1),
    SD = round(sd(errors), 2)
  )
```

Yes, they do!


### Anova on full 4 (conditions) x 2 (word categories) design:

```{r}
# Following this post:
# https://www.r-bloggers.com/two-way-anova-with-repeated-measures/
aov_orig_allconds <- aov(
  errors ~ movement * word_type +
    Error(subject / (movement * word_type)),
  data = d_all_conds)
summary(aov_orig_allconds)
```


```{r}
aov_orig <- aov(errors ~ movement * word_type +
                  Error(subject / (movement * word_type)),
                data = d)
summary(aov_orig)
```



Treat data as coming from a binomial distribution, but what is $n$?
======================================================

SP13 analyzed the data using ANOVAs and t-tests. We instead want to model the
dependent variable (DV = number of errors per experimental cell) as coming from
a binomial distribution: 

$$errors \sim B(n, p)$$

Here, $n$ is the upper bound on the possible number of errors and $p$ is the
probability of making an error for each word.
We will assume that the maximum number of errors for each trial (i.e., sequence
of 4 words) is 4.
However, we also need to determine how many trials there were per cell and this
is slightly tricky, because SP13 report this inconsistently.

## Email correspondence with authors

We emailed the original authors about this. Here is the relevant extract, with 
the authors' responses as inline comments (email correspondence from 1 Apr 2018):

> **Number of trials**
>
> It says that 24 trials were presented in each block, twelve arm-word and 12 leg-word trials (sect. 2.3, p.225 left
column [=LC]). It also says that 4 words were presented per trial (sect. 2.3, p.224 right column [=RC]). This implies
that 48 words of each category were shown per block. Since the lists consist of 36 words per category, the above
would suggest that some words were repeated in each block (e.g., 12 words per category repeated once), but this is
not explicitly stated. Was this the case?
>
>> Yes, 48 words from each category were shown in each block. Twelve words per category, randomly selected, were
repeated once in each block.
>
> It then says that conditions were run as separate blocks (sect. 2.3, p.225 LC) and that the full set of 72 words were
presented twice in each condition (sect. 2.3, p.225 RC). These figures don’t seem to add up: For 72 * 2 = 144 words
to be presented, this would require 144 / 4 = 36 trials (assuming 4 words are presented in each trial). In other words,
one would need 1.5 blocks per condition, whereas in the study it says that “the conditions were run as separate
blocks with twenty-four trials in each block” (p.225). Is there perhaps a typo in the reported numbers, or do we err
in our reasoning? In sum, what was the exact number of words per trial, trials per block (we assume equal number
of leg- and arm-word trials), and blocks per condition?
>
>> Line 2 of page 225, RC does indeed contain a typo. As mentioned above, 48 words from each category were
presented in each block (36 +12 = 48), therefore, not all words were repeated twice in each block/condition.
In sum:
Words per trial: 4
Trials per block: 24 (12 arm word and 12 leg word trials)
Blocks in the experiment: 4
Blocks per condition: 1


## We set $n = 48$

Based on this correspondence, we take there to be 12 trials per experimental
cell. Since we assume the maximum number of errors per trial is 4,
$n = 12 \times 4 = 48$.

```{r}
d$n <- 48
```



GLMM with *lme4*
================

We start by fitting a frequentist GLMM with *lme4* for a first quick
interpretation of the estimated model coefficients.

Coding scheme
-------------

Set coding scheme to contrast coding:

```{r}
# movement condition
d$movement <- factor(d$movement)
contrasts(d$movement) <- contr.sum(2)
colnames(contrasts(d$movement)) <- "arm_vs_leg"
contrasts(d$movement)
# word type
d$word_type <- factor(d$word_type)
contrasts(d$word_type) <- contr.sum(2)
colnames(contrasts(d$word_type)) <- "arm_vs_leg"
contrasts(d$word_type)
```


Binomial GLMM
-------------

```{r}
fm_binom <- glmer(cbind(errors, n - errors) ~ 1 + movement * word_type +
                    (1 | subject),
                  data = d, family = "binomial")
summary(fm_binom)
```

Interpretation (backtransforming to odds):

- The estimated average odds of an error (intercept) is 
$e ^ {`r round(fixef(fm_binom)[1], 2)`} = `r round(exp(fixef(fm_binom)[1]), 2)`$
(corresponding to a probability of
`r round(inv.logit(fixef(fm_binom)[1]), 2)`)
- Neither the type of movement (`movement`) or the type of word (`word_type`)
have a significant effect on the odds of making an error.
- The critical interaction is significant and tells us that the odds of making
an error if the effector of the movement *coincides* with the word type (as
opposed to when the two differ) is
$e ^ {`r round(fixef(fm_binom)[4], 2)`} =`r round(exp(fixef(fm_binom)[4]), 2)`$.
This is the *interference effect* of interest!



Bayesian GLMM with *brms*
=========================

We fit a binomial model using all of the default options for priors, etc.
(For the syntax of how to specify the response variable in a binomial model,
see Bürkner 2017, p.7, fn 3.)

```{r}
# bfm_binom <- brm(errors | trials(n) ~ 1 + movement * word_type + (1 | subject),
#                  data = d,
#                  family = "binomial",
#                  warmup = 2000, iter = 7000)
```

```{r}
# To save time the fitted model has been saved to file
# saveRDS(bfm_binom, "sims_etc/bayes_glmm_default.rds")
bfm_binom <- readRDS("sims_etc/bayes_glmm_default.rds")
```



```{r}
summary(bfm_binom)
```

The point estimates are virtually identical in the *brms* and *lme4* models.
The *brms* model also features 95% credible intervals for parameter estimates.
The 95% credible interval for the critical interaction
$[`r round(fixef(bfm_binom)[4,3], 2)`, `r round(fixef(bfm_binom)[4,4], 2)`]$
clearly contains the point estimate from the *lme4* model, and the two are 
identical
(*brms*: $`r round(fixef(bfm_binom)[4,1], 4)`;
*lme4*: `r round(fixef(fm_binom)[4], 4)`$).
Note that the estimated SEM also are quite comparable.

*brms* allows us to perform "non-linear hypothesis testing". Let us test the
hypothesis that the crucial interaction is larger than zero (i.e., there is an
interference effect):

```{r}
hypothesis(bfm_binom, "movementarm_vs_leg:word_typearm_vs_leg > 0")
```

The evidence ratio for an interference effect is overwhelming.

We can also plot the posterior distribution of the model estimates:

```{r, fig.height=7}
plot(bfm_binom)
```



Use posterior distribution of interaction coefficient as effect size prior for BFDA
============================================

*brms* has a `posterior_samples(x, pars)` function to get posterior traces for
specific coefficients. We pass our fitted *brms* model as x, and the predictor
of interest as pars, and obtain a dataframe with the samples in return.

This posterior from the original we can then use as the prior for the design
analysis in our replication.

```{r}
beta_posterior <- posterior_samples(bfm_binom, "movementarm_vs_leg:word_typearm_vs_leg")
write_csv(beta_posterior, "sims_etc/beta_posterior.csv")
```


Is the effect we find in the sample robust? Apply bootstrapping!
============================

Markus suggested that the apparently robust interaction effect observed in the
original data might hinge on just a few participants who are driving the effect. 
One way of assessing this is through bootstrapping.

Bootstrap procedure:

1. Take a random sample of size 15 with replacement from the 15 participants in
the original study.
2. Analyze the data set with the *lme4* model above (faster than *brms*).
3. Extract the estimates for the coefficient and its SE.
4. Repeat steps 1--3 a large number of times (e.g., 5000)


Basic function:

```{r}
# Function for a single run:
mybootstr <- function(df = d) {

  # Create new data set by resampling Subjects with replacement
  # (NB: Relies on dplyr functions, see in particular nest())
  newd <- df %>%
    nest(-subject) %>%
    sample_n(15, replace = TRUE) %>%
    mutate(subject = 1 : 15) %>%  # assign each resampled S a unique ID
    unnest()

  # contrast coding for movement and wordtype
  newd$movement <- factor(newd$movement)
  contrasts(newd$movement) <- contr.sum(2)
  newd$word_type <- factor(newd$word_type)
  contrasts(newd$word_type) <- contr.sum(2)

  # Fit glmer model
  fm_binom <- glmer(cbind(errors, n - errors) ~ 1 + movement * word_type +
                      (1 | subject),
                    data = newd, family = "binomial")

  # extract point estimate and SE for interaction coefficient of interest, based
  # on https://r.789695.n4.nabble.com/extracting-coefficients-from-lmer-td791247.html
  beta <- fixef(fm_binom)[4]
  SE   <- sqrt(diag(vcov(fm_binom, useScale = FALSE))[4])
  zval <- beta / SE
  # out
  out <- c(beta, SE, zval)
  names(out) <- c("beta", "SE", "zval")
  out
}
# Run once
mybootstr()
# Using replicate
data.frame(t(replicate(4, mybootstr())))
```

Run this function a large number of times and save results:

```{r}
# # Run and save to disk
# bootstr_res <- data.frame(t(replicate(5000, mybootstr())))
# write_csv(bootstr_res, "sims_etc/bootstrap_results.csv")
```

Once run, read from disk:

```{r}
# Read from disk
bootstr_res <- read_csv("sims_etc/bootstrap_results.csv")
str(bootstr_res)
head(bootstr_res)
```


Plot results:

```{r}
ggplot(bootstr_res, aes(x = beta)) +
  geom_density() + xlim(0, 0.3)
ggplot(bootstr_res, aes(x = SE)) +
  geom_density()
ggplot(bootstr_res, aes(x = zval)) +
  geom_density() + xlim(1, 10) +
  geom_vline(xintercept = 1.96, linetype = "dashed")
```


Proportion of runs in which $z < 1.96$:

```{r}
with(bootstr_res, sum(zval < 1.96) / length(zval))
```



Session info
============

```{r}
sessionInfo()
```

